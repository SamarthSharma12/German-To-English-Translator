{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import load,dump\n",
        "\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        "from numpy import argmax\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "metadata": {
        "id": "6clWSy6X90RI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, mode='rt', encoding='utf-8')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text"
      ],
      "metadata": {
        "id": "TxWL691O90yc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_pairs(doc):\n",
        "    lines = doc.strip().split('\\n')\n",
        "    pairs = [line.split('\\t') for line in  lines]\n",
        "    return pairs"
      ],
      "metadata": {
        "id": "6zqmeLny900w"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "    cleaned = list()\n",
        "    # prepare regex for char filtering\n",
        "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "    # prepare translation table for removing punctuation\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    for pair in lines:\n",
        "        clean_pair = list()\n",
        "        for line in pair:\n",
        "            # normalize unicode characters\n",
        "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "            line = line.decode('UTF-8')\n",
        "            # tokenize on white space\n",
        "            line = line.split()\n",
        "            # convert to lowercase\n",
        "            line = [word.lower() for word in line]\n",
        "            # remove punctuation from each token\n",
        "            line = [word.translate(table) for word in line]\n",
        "            # remove non-printable chars form each token\n",
        "            line = [re_print.sub('', w) for w in line]\n",
        "            # remove tokens with numbers in them\n",
        "            line = [word for word in line if word.isalpha()]\n",
        "            # store as string\n",
        "            clean_pair.append(' '.join(line))\n",
        "        cleaned.append(clean_pair)\n",
        "    return array(cleaned)"
      ],
      "metadata": {
        "id": "5no1b-eG9028"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_clean_data(sentences, filename):\n",
        "    dump(sentences, open(filename, 'wb'))\n",
        "    print('Saved: %s' % filename)"
      ],
      "metadata": {
        "id": "A87MMFzu906L"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "filename = 'deu.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into english-german pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, 'english-german.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGSHflnFxU4_",
        "outputId": "a87c0f2d-25e5-4953-e47c-0e40c02b1139"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: english-german.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "    return load(open(filename, 'rb'))"
      ],
      "metadata": {
        "id": "FHzvVn4PxVxn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('english-german.pkl')\n",
        "\n",
        "# reduce dataset size\n",
        "n_sentences = 10000\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "train, test = dataset[:9000], dataset[9000:]\n",
        "# save\n",
        "save_clean_data(dataset, 'english-german-both.pkl')\n",
        "save_clean_data(train, 'english-german-train.pkl')\n",
        "save_clean_data(test, 'english-german-test.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odcgb3-m0Qif",
        "outputId": "5612debe-3ba4-4a6b-8f14-5e38c9139061"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: english-german-both.pkl\n",
            "Saved: english-german-train.pkl\n",
            "Saved: english-german-test.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "    return load(open(filename, 'rb'))\n",
        "    "
      ],
      "metadata": {
        "id": "-bLZ2Mzc0Sah"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('english-german.pkl')\n",
        "\n",
        "# reduce dataset size\n",
        "n_sentences = 10000\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "train, test = dataset[:9000], dataset[9000:]\n",
        "# save\n",
        "save_clean_data(dataset, 'english-german-both.pkl')\n",
        "save_clean_data(train, 'english-german-train.pkl')\n",
        "save_clean_data(test, 'english-german-test.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88kJhGQZ0ar3",
        "outputId": "ef6417e1-ff3f-4bce-a5c1-b5f4900e07f0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: english-german-both.pkl\n",
            "Saved: english-german-train.pkl\n",
            "Saved: english-german-test.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tokenizer(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "_LpmYYxp0coj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def max_length(lines):\n",
        "    return max(len(line.split()) for line in lines)"
      ],
      "metadata": {
        "id": "FnVNVt8e0fkm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_sequences(tokenizer, length, lines):\n",
        "    X = tokenizer.texts_to_sequences(lines)\n",
        "    # pad sequences with 0 values\n",
        "    X = pad_sequences(X, maxlen=length, padding='post')\n",
        "    return X\n",
        "\n",
        "def encode_output(sequences, vocab_size):\n",
        "    ylist = list()\n",
        "    for sequence in sequences:\n",
        "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "        ylist.append(encoded)\n",
        "    y = array(ylist)\n",
        "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "    return y"
      ],
      "metadata": {
        "id": "HcqsDagm0guv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "    model.add(LSTM(n_units))\n",
        "    model.add(RepeatVector(tar_timesteps))\n",
        "    model.add(LSTM(n_units, return_sequences=True))\n",
        "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "    return model"
      ],
      "metadata": {
        "id": "bkCVE1g_0iyR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load datasets\n",
        "dataset = load_clean_sentences('english-german-both.pkl')\n",
        "train = load_clean_sentences('english-german-train.pkl')\n",
        "test = load_clean_sentences('english-german-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('German Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLRFOl7P0lH2",
        "outputId": "efc25ec6-f7e2-4e19-f0d1-0f8d61663890"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Vocabulary Size: 2404\n",
            "English Max Length: 5\n",
            "German Vocabulary Size: 3856\n",
            "German Max Length: 10\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 10, 256)           987136    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 256)               525312    \n",
            "                                                                 \n",
            " repeat_vector (RepeatVector  (None, 5, 256)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 5, 256)            525312    \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, 5, 2404)          617828    \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,655,588\n",
            "Trainable params: 2,655,588\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/30\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 3.69747, saving model to model.h5\n",
            "141/141 - 18s - loss: 4.4455 - val_loss: 3.6975 - 18s/epoch - 129ms/step\n",
            "Epoch 2/30\n",
            "\n",
            "Epoch 2: val_loss improved from 3.69747 to 3.53902, saving model to model.h5\n",
            "141/141 - 3s - loss: 3.5453 - val_loss: 3.5390 - 3s/epoch - 22ms/step\n",
            "Epoch 3/30\n",
            "\n",
            "Epoch 3: val_loss improved from 3.53902 to 3.44488, saving model to model.h5\n",
            "141/141 - 2s - loss: 3.3876 - val_loss: 3.4449 - 2s/epoch - 15ms/step\n",
            "Epoch 4/30\n",
            "\n",
            "Epoch 4: val_loss improved from 3.44488 to 3.36919, saving model to model.h5\n",
            "141/141 - 2s - loss: 3.2352 - val_loss: 3.3692 - 2s/epoch - 12ms/step\n",
            "Epoch 5/30\n",
            "\n",
            "Epoch 5: val_loss improved from 3.36919 to 3.25636, saving model to model.h5\n",
            "141/141 - 2s - loss: 3.1035 - val_loss: 3.2564 - 2s/epoch - 13ms/step\n",
            "Epoch 6/30\n",
            "\n",
            "Epoch 6: val_loss improved from 3.25636 to 3.15719, saving model to model.h5\n",
            "141/141 - 3s - loss: 2.9720 - val_loss: 3.1572 - 3s/epoch - 19ms/step\n",
            "Epoch 7/30\n",
            "\n",
            "Epoch 7: val_loss improved from 3.15719 to 3.05847, saving model to model.h5\n",
            "141/141 - 2s - loss: 2.8332 - val_loss: 3.0585 - 2s/epoch - 12ms/step\n",
            "Epoch 8/30\n",
            "\n",
            "Epoch 8: val_loss improved from 3.05847 to 2.95430, saving model to model.h5\n",
            "141/141 - 2s - loss: 2.6973 - val_loss: 2.9543 - 2s/epoch - 14ms/step\n",
            "Epoch 9/30\n",
            "\n",
            "Epoch 9: val_loss improved from 2.95430 to 2.85511, saving model to model.h5\n",
            "141/141 - 2s - loss: 2.5520 - val_loss: 2.8551 - 2s/epoch - 14ms/step\n",
            "Epoch 10/30\n",
            "\n",
            "Epoch 10: val_loss improved from 2.85511 to 2.73101, saving model to model.h5\n",
            "141/141 - 2s - loss: 2.3865 - val_loss: 2.7310 - 2s/epoch - 14ms/step\n",
            "Epoch 11/30\n",
            "\n",
            "Epoch 11: val_loss improved from 2.73101 to 2.64680, saving model to model.h5\n",
            "141/141 - 2s - loss: 2.2246 - val_loss: 2.6468 - 2s/epoch - 11ms/step\n",
            "Epoch 12/30\n",
            "\n",
            "Epoch 12: val_loss improved from 2.64680 to 2.58850, saving model to model.h5\n",
            "141/141 - 3s - loss: 2.0783 - val_loss: 2.5885 - 3s/epoch - 18ms/step\n",
            "Epoch 13/30\n",
            "\n",
            "Epoch 13: val_loss improved from 2.58850 to 2.52156, saving model to model.h5\n",
            "141/141 - 2s - loss: 1.9467 - val_loss: 2.5216 - 2s/epoch - 13ms/step\n",
            "Epoch 14/30\n",
            "\n",
            "Epoch 14: val_loss improved from 2.52156 to 2.46412, saving model to model.h5\n",
            "141/141 - 2s - loss: 1.8231 - val_loss: 2.4641 - 2s/epoch - 11ms/step\n",
            "Epoch 15/30\n",
            "\n",
            "Epoch 15: val_loss improved from 2.46412 to 2.39949, saving model to model.h5\n",
            "141/141 - 2s - loss: 1.7119 - val_loss: 2.3995 - 2s/epoch - 11ms/step\n",
            "Epoch 16/30\n",
            "\n",
            "Epoch 16: val_loss improved from 2.39949 to 2.35315, saving model to model.h5\n",
            "141/141 - 2s - loss: 1.6038 - val_loss: 2.3532 - 2s/epoch - 12ms/step\n",
            "Epoch 17/30\n",
            "\n",
            "Epoch 17: val_loss improved from 2.35315 to 2.32276, saving model to model.h5\n",
            "141/141 - 2s - loss: 1.5012 - val_loss: 2.3228 - 2s/epoch - 11ms/step\n",
            "Epoch 18/30\n",
            "\n",
            "Epoch 18: val_loss improved from 2.32276 to 2.28510, saving model to model.h5\n",
            "141/141 - 2s - loss: 1.4050 - val_loss: 2.2851 - 2s/epoch - 11ms/step\n",
            "Epoch 19/30\n",
            "\n",
            "Epoch 19: val_loss improved from 2.28510 to 2.26860, saving model to model.h5\n",
            "141/141 - 3s - loss: 1.3162 - val_loss: 2.2686 - 3s/epoch - 19ms/step\n",
            "Epoch 20/30\n",
            "\n",
            "Epoch 20: val_loss improved from 2.26860 to 2.22333, saving model to model.h5\n",
            "141/141 - 2s - loss: 1.2255 - val_loss: 2.2233 - 2s/epoch - 13ms/step\n",
            "Epoch 21/30\n",
            "\n",
            "Epoch 21: val_loss improved from 2.22333 to 2.21534, saving model to model.h5\n",
            "141/141 - 2s - loss: 1.1411 - val_loss: 2.2153 - 2s/epoch - 11ms/step\n",
            "Epoch 22/30\n",
            "\n",
            "Epoch 22: val_loss improved from 2.21534 to 2.20225, saving model to model.h5\n",
            "141/141 - 2s - loss: 1.0584 - val_loss: 2.2022 - 2s/epoch - 11ms/step\n",
            "Epoch 23/30\n",
            "\n",
            "Epoch 23: val_loss improved from 2.20225 to 2.17686, saving model to model.h5\n",
            "141/141 - 2s - loss: 0.9860 - val_loss: 2.1769 - 2s/epoch - 12ms/step\n",
            "Epoch 24/30\n",
            "\n",
            "Epoch 24: val_loss improved from 2.17686 to 2.15176, saving model to model.h5\n",
            "141/141 - 2s - loss: 0.9129 - val_loss: 2.1518 - 2s/epoch - 11ms/step\n",
            "Epoch 25/30\n",
            "\n",
            "Epoch 25: val_loss improved from 2.15176 to 2.14753, saving model to model.h5\n",
            "141/141 - 2s - loss: 0.8451 - val_loss: 2.1475 - 2s/epoch - 12ms/step\n",
            "Epoch 26/30\n",
            "\n",
            "Epoch 26: val_loss improved from 2.14753 to 2.14621, saving model to model.h5\n",
            "141/141 - 2s - loss: 0.7820 - val_loss: 2.1462 - 2s/epoch - 17ms/step\n",
            "Epoch 27/30\n",
            "\n",
            "Epoch 27: val_loss improved from 2.14621 to 2.13771, saving model to model.h5\n",
            "141/141 - 2s - loss: 0.7206 - val_loss: 2.1377 - 2s/epoch - 12ms/step\n",
            "Epoch 28/30\n",
            "\n",
            "Epoch 28: val_loss did not improve from 2.13771\n",
            "141/141 - 2s - loss: 0.6702 - val_loss: 2.1443 - 2s/epoch - 13ms/step\n",
            "Epoch 29/30\n",
            "\n",
            "Epoch 29: val_loss did not improve from 2.13771\n",
            "141/141 - 2s - loss: 0.6155 - val_loss: 2.1420 - 2s/epoch - 14ms/step\n",
            "Epoch 30/30\n",
            "\n",
            "Epoch 30: val_loss improved from 2.13771 to 2.13090, saving model to model.h5\n",
            "141/141 - 2s - loss: 0.5678 - val_loss: 2.1309 - 2s/epoch - 11ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8f72cafee0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def word_for_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None"
      ],
      "metadata": {
        "id": "2AgfoGea0mP3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sequence(model, tokenizer, source):\n",
        "    prediction = model.predict(source, verbose=0)[0]\n",
        "    integers = [argmax(vector) for vector in prediction]\n",
        "    target = list()\n",
        "    for i in integers:\n",
        "        word = word_for_id(i, tokenizer)\n",
        "        if word is None:\n",
        "            break\n",
        "        target.append(word)\n",
        "    return ' '.join(target)"
      ],
      "metadata": {
        "id": "UzYyDk3z1Dcy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "    actual, predicted = list(), list()\n",
        "    for i, source in enumerate(sources):\n",
        "        # translate encoded source text\n",
        "        source = source.reshape((1, source.shape[0]))\n",
        "        translation = predict_sequence(model, eng_tokenizer, source)\n",
        "        raw_target, raw_src = raw_dataset[i]\n",
        "        if i < 10:\n",
        "            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "        actual.append([raw_target.split()])\n",
        "        predicted.append(translation.split())\n",
        "    # calculate BLEU score\n",
        "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
      ],
      "metadata": {
        "id": "T5qsxZII1Ej5"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-3REycF1GNP",
        "outputId": "ef060836-d986-46e3-e819-3f436e146ab6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train\n",
            "src=[sie ist geschieden], target=[shes divorced], predicted=[shes divorced]\n",
            "src=[er ist hundemude], target=[hes deadtired], predicted=[hes deadtired]\n",
            "src=[das ist nicht genug], target=[its not enough], predicted=[its not stupid]\n",
            "src=[geh es suchen], target=[go look for it], predicted=[go and for it]\n",
            "src=[ich fahre gern ski], target=[i like skiing], predicted=[i like skiing]\n",
            "src=[haben sie reis], target=[do you have rice], predicted=[do you have rice]\n",
            "src=[das leben ist verruckt], target=[life is crazy], predicted=[life is crazy]\n",
            "src=[ich bin ungarin], target=[i am hungarian], predicted=[i am hungarian]\n",
            "src=[tom ist stark], target=[tom is strong], predicted=[toms is]\n",
            "src=[beeilt euch madels], target=[hurry up girls], predicted=[hurry up girls]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "22Acsxg_1H6v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}